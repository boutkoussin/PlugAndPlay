\relax 
\citation{CaseyOpticallyCoherent}
\@writefile{toc}{\contentsline {section}{\numberline {1}Case I: Linear measurements $(A=I)$}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Forward-model: }{1}}
\newlabel{fig:forwardModel}{{1.1}{1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:reflectanceObject}{{1a}{1}}
\newlabel{sub@fig:reflectanceObject}{{a}{1}}
\newlabel{fig:opticalField}{{1b}{1}}
\newlabel{sub@fig:opticalField}{{b}{1}}
\newlabel{fig:noisyMeasurement1e-2}{{1c}{1}}
\newlabel{sub@fig:noisyMeasurement1e-2}{{c}{1}}
\newlabel{fig:noisyMeasurement1e-1}{{1d}{1}}
\newlabel{sub@fig:noisyMeasurement1e-1}{{d}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An example-realization of the forward-model $y=g+w$ . Two realizations of the noisy observations are shown at std. deviations of $\{0.001, 0.1\}$.\relax }}{1}}
\newlabel{fig:forwardModel}{{1}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}ML-estimate}{1}}
\citation{PnPalgorithm}
\newlabel{fig:ML-2}{{2a}{2}}
\newlabel{sub@fig:ML-2}{{a}{2}}
\newlabel{fig:ML-1}{{2b}{2}}
\newlabel{sub@fig:ML-1}{{b}{2}}
\newlabel{fig:ML0}{{2c}{2}}
\newlabel{sub@fig:ML0}{{c}{2}}
\newlabel{fig:ML1}{{2d}{2}}
\newlabel{sub@fig:ML1}{{d}{2}}
\newlabel{fig:PnP-2}{{2e}{2}}
\newlabel{sub@fig:PnP-2}{{e}{2}}
\newlabel{fig:PnP-1}{{2f}{2}}
\newlabel{sub@fig:PnP-1}{{f}{2}}
\newlabel{fig:PnP0}{{2g}{2}}
\newlabel{sub@fig:PnP0}{{g}{2}}
\newlabel{fig:PnP1}{{2h}{2}}
\newlabel{sub@fig:PnP1}{{h}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An example-realization for different inversion-models. The inversions are shown for different std. deviations of $\{1e^{-4},1e^{-3}, 1e^{-2},1e^{-1}\}$.\relax }}{2}}
\newlabel{fig:inversionModel}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Plug and play (PnP) estimate (MAP)}{2}}
\newlabel{eq:MAPestimate}{{1}{2}}
\newlabel{fig:MLN1}{{3a}{3}}
\newlabel{sub@fig:MLN1}{{a}{3}}
\newlabel{fig:MLN3}{{3b}{3}}
\newlabel{sub@fig:MLN3}{{b}{3}}
\newlabel{fig:MLN20}{{3c}{3}}
\newlabel{sub@fig:MLN20}{{c}{3}}
\newlabel{fig:MLN40}{{3d}{3}}
\newlabel{sub@fig:MLN40}{{d}{3}}
\newlabel{fig:PnP-2}{{3e}{3}}
\newlabel{sub@fig:PnP-2}{{e}{3}}
\newlabel{fig:PnP-1}{{3f}{3}}
\newlabel{sub@fig:PnP-1}{{f}{3}}
\newlabel{fig:PnP0}{{3g}{3}}
\newlabel{sub@fig:PnP0}{{g}{3}}
\newlabel{fig:PnP1}{{3h}{3}}
\newlabel{sub@fig:PnP1}{{h}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Multi-image plug and play algorithm for: For the image with $\sigma _w=1e^{-3}$ .\relax }}{3}}
\newlabel{fig:inversionModel}{{3}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.3.1}Parameter-tuning}{3}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Plug and Play ($y, \sigma _w,\sigma _\lambda ,\sigma _n$)\relax }}{4}}
\newlabel{euclid}{{1}{4}}
\newlabel{alg:PnP}{{1}{4}}
\citation{RED}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Regularization by denoising}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5}Properties of the denosier $f(x)$}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6}Solving the regularized optimization problem}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7}Regularization by denoising for reflectance-estimation }{5}}
\citation{SRNET}
\bibcite{CaseyOpticallyCoherent}{1}
\bibcite{PnPalgorithm}{2}
\bibcite{RED}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A sample residual and conolutional block.\relax }}{7}}
\newlabel{residualConvBlock}{{4}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Network architecture used to generate results in Figure\nobreakspace  {}5\hbox {}.\relax }}{7}}
\newlabel{networkArch}{{1}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.8}Direct CNN-approach}{7}}
\bibcite{SRNET}{4}
\newlabel{fig:GT}{{5a}{8}}
\newlabel{sub@fig:GT}{{a}{8}}
\newlabel{fig:inputImg}{{5b}{8}}
\newlabel{sub@fig:inputImg}{{b}{8}}
\newlabel{fig:outputImg}{{5c}{8}}
\newlabel{sub@fig:outputImg}{{c}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The learning-rate used to define this network is $0.01$ with a step-decay parameter of $0.1$ and a step-list being $[10, 25]$. The number of training-epochs is $100$.\relax }}{8}}
\newlabel{fig:outputFig}{{5}{8}}
\newlabel{fig:GT}{{6a}{8}}
\newlabel{sub@fig:GT}{{a}{8}}
\newlabel{fig:inputImg}{{6b}{8}}
\newlabel{sub@fig:inputImg}{{b}{8}}
\newlabel{fig:outputImg}{{6c}{8}}
\newlabel{sub@fig:outputImg}{{c}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The learning-rate used to define this network is $0.01$ with a step-decay parameter of $0.1$ and a step-list being $[10, 25]$. The number of training-epochs is $100$. We use a deeper-architecture in comparison to the SRNET architecture.\relax }}{8}}
\newlabel{fig:outputFig}{{6}{8}}
